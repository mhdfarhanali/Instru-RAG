{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7da295bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mhdfarhanali/Documents/Instru RAG/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb41606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panjang teks (karakter): 670951\n",
      "Cuplikan awal teks:\n",
      " --- [HALAMAN 1] --- This E-Book and More From http://ali-almukhtar.blogspot.com --- [HALAMAN 2] --- Introduction to Instrumentation, Sensors, and Process Control --- [HALAMAN 3] --- For a listing of related titles from Artech House, turn to the back of this book --- [HALAMAN 4] --- Introduction to Instrumentation, Sensors, and Process Control William C. Dunn artechhouse.com --- [HALAMAN 5] --- Library of Congress Cataloging-in-Publication Data Dunn, William C. Introduction to instrumentation, se\n"
     ]
    }
   ],
   "source": [
    "# Path file hasil ekstraksi PDF\n",
    "data_path = \"/Users/mhdfarhanali/Documents/InstruRAG/data/raw/instrumentation_sensors.txt\"\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Panjang teks (karakter):\", len(raw_text))\n",
    "print(\"Cuplikan awal teks:\\n\", raw_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db709510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah chunks: 1677\n",
      "Contoh chunk:\n",
      " --- [HALAMAN 1] --- This E-Book and More From http://ali-almukhtar.blogspot.com --- [HALAMAN 2] --- Introduction to Instrumentation, Sensors, and Process Control --- [HALAMAN 3] --- For a listing of related titles from Artech House, turn to the back of this book --- [HALAMAN 4] --- Introduction to I\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = text_splitter.split_text(raw_text)\n",
    "\n",
    "print(f\"Jumlah chunks: {len(chunks)}\")\n",
    "print(\"Contoh chunk:\\n\", chunks[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3184c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 53/53 [00:05<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding selesai!\n",
      "Jumlah chunks: 1677\n",
      "Dimensi vektor: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Encode + normalisasi\n",
    "embeddings = embedding_model.encode(\n",
    "    chunks,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(\"Embedding selesai!\")\n",
    "print(f\"Jumlah chunks: {len(chunks)}\")\n",
    "print(f\"Dimensi vektor: {embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40195c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koleksi 'instru_collection' sudah ada\n",
      "Data berhasil diunggah ke Qdrant\n"
     ]
    }
   ],
   "source": [
    "# Hubungkan ke Qdrant\n",
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "qdrant = QdrantClient(host=\"localhost\", port=6333)\n",
    "collection_name = \"instru_collection\"\n",
    "\n",
    "# Buat koleksi jika belum ada\n",
    "try:\n",
    "    qdrant.get_collection(collection_name)\n",
    "    print(f\"Koleksi '{collection_name}' sudah ada\")\n",
    "except:\n",
    "    qdrant.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=embeddings.shape[1],\n",
    "            distance=models.Distance.COSINE\n",
    "        )\n",
    "    )\n",
    "    print(f\"Koleksi '{collection_name}' berhasil dibuat\")\n",
    "\n",
    "# Filter chunk kosong dulu\n",
    "chunks = [c for c in chunks if c and c.strip()]\n",
    "\n",
    "# Siapkan data points\n",
    "points = [\n",
    "    models.PointStruct(\n",
    "        id=i,\n",
    "        vector=embeddings[i],\n",
    "        payload={\"page_content\": str(chunks[i])} \n",
    "    )\n",
    "    for i in range(len(chunks))\n",
    "]\n",
    "\n",
    "# Upload data ke Qdrant\n",
    "qdrant.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n",
    "print(\"Data berhasil diunggah ke Qdrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c5de5c",
   "metadata": {},
   "source": [
    "# Interpretasi \n",
    "\n",
    "1. Tujuan\n",
    "Notebook ini digunakan untuk mengolah teks hasil ekstraksi PDF yang sebelumnya dibuat di 0_extract_pdf.ipynb.\n",
    "Tujuan utamanya adalah membagi teks yang sangat panjang menjadi potongan kecil (chunks) agar bisa dipahami dan diproses oleh model AI secara efisien.\n",
    "Tahap ini penting karena model AI seperti LLM (Large Language Model) tidak bisa langsung membaca teks yang terlalu panjang, sehingga kita harus memecahnya menjadi bagian-bagian kecil tapi tetap bermakna.\n",
    "Selain itu, di tahap ini juga dilakukan proses penyimpanan data hasil pemecahan ke dalam database vektor (Qdrant) agar nanti bisa dicari kembali saat chatbot menjawab pertanyaan.\n",
    "\n",
    "2. Proses yang Dilakukan\n",
    "Langkah-langkah utama di notebook ini adalah:\n",
    "\n",
    "a. Membaca File Hasil Ekstraksi\n",
    "Teks yang dihasilkan dari notebook sebelumnya (instrumentation_sensors.txt) dibuka dan dibaca sepenuhnya.\n",
    "Dari hasil pembacaan, diketahui panjang teks mencapai 670.951 karakter — ini menandakan bahwa isi buku sudah lengkap dan berhasil terbaca dengan baik.\n",
    "Cuplikan awal teks yang terlihat berisi bagian pembuka dari buku seperti halaman judul, informasi penerbit, dan keterangan katalog.\n",
    "\n",
    "b. Memecah Teks Menjadi Potongan (Chunks)\n",
    "Langkah berikutnya adalah membagi teks panjang tersebut menjadi potongan-potongan kecil agar lebih mudah dikelola.\n",
    "Untuk itu digunakan RecursiveCharacterTextSplitter dari library langchain_text_splitters.\n",
    "Setiap potongan memiliki ukuran sekitar 500 karakter, dan setiap potongan saling tumpang tindih sedikit sebanyak 100 karakter.\n",
    "Overlap ini berguna agar tidak ada konteks penting yang terpotong di antara dua potongan teks.\n",
    "Setelah proses pemecahan selesai, didapatkan hasil:\n",
    "Jumlah total chunks: 1677\n",
    "Contoh isi chunk pertama: bagian pembuka dari buku (judul, pengantar, dan informasi awal).\n",
    "\n",
    "c. Menyimpan ke Database Qdrant\n",
    "Setelah semua chunks terbentuk, langkah berikutnya adalah menyimpan data hasil potongan ini ke dalam database vektor (Qdrant).\n",
    "Qdrant digunakan karena mampu menyimpan data dalam bentuk vektor yang bisa dicari dengan metode similarity search.\n",
    "Artinya, ketika nanti chatbot menerima pertanyaan, sistem akan mencari potongan teks yang paling relevan di database Qdrant.\n",
    "Pesan di terminal menunjukkan bahwa koleksi (collection) bernama \"instru_collection\" sudah ada, sehingga data baru bisa langsung ditambahkan tanpa perlu membuat ulang koleksi.\n",
    "Hasil akhirnya muncul pesan:\n",
    "Koleksi 'instru_collection' sudah ada\n",
    "Data berhasil diunggah ke Qdrant\n",
    "Ini berarti data sudah tersimpan dengan baik di database dan siap digunakan untuk tahap berikutnya, yaitu indexing dan retrieval.\n",
    "\n",
    "3. Hasil Akhir\n",
    "Panjang teks mentah: 670.951 karakter\n",
    "Total chunks yang dihasilkan: 1677 potongan teks\n",
    "Database vektor (Qdrant) berhasil diisi dengan semua potongan teks\n",
    "Tidak ada error selama proses\n",
    "Secara keseluruhan, proses berjalan sangat lancar dan hasilnya sesuai harapan.\n",
    "\n",
    "4. Kesimpulan\n",
    "Tahap 1_ingestion.ipynb berhasil mempersiapkan data agar bisa digunakan dalam sistem RAG.\n",
    "Sekarang data buku sudah dipecah menjadi ribuan potongan kecil yang bisa dicari secara cepat dan akurat oleh model AI.\n",
    "Dengan langkah ini, chatbot Instru RAG nanti bisa menjawab pertanyaan berdasarkan isi buku karena semua informasi teknis sudah tersimpan dalam bentuk vektor di Qdrant.\n",
    "\n",
    "## Catatan Pribadi\n",
    "Menurut saya, tahap ini seperti memotong buku menjadi potongan-potongan kecil supaya bisa “dimakan” oleh AI.\n",
    "Awalnya saya agak bingung kenapa teks harus dipecah-pecah, tapi setelah dijelaskan, ternyata ini penting karena model AI punya batas panjang input.\n",
    "Bagian paling menarik adalah saat data diunggah ke Qdrant dan muncul pesan “Data berhasil diunggah ke Qdrant”.\n",
    "Itu artinya sistem saya mulai “punya memori”, dan dari sinilah chatbot bisa mulai belajar menjawab dari isi buku teknik asli."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
